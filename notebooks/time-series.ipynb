{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### usual imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fbprophet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fbprophet](https://facebook.github.io/prophet/) implement a [Generalized Additive Model](https://en.wikipedia.org/wiki/Generalized_additive_model) in Python and R, and is extremely convenient for the modelling of time-series which contain a **trend** (potentially non-linear), some **cyclic components** (seasonal cycle, weekly cycle, daily cycle) and potentially respond\n",
    "to **`pulse` events** which do not obey a regular schedule, the latter is particularly important for e.g. time-series of sales data, where holidays, special events, marketing campains, etc, might drive short term, irregular increases or decrease in sales volume / value.   \n",
    "\n",
    "And of course, of interest for us is the fact that you can easily add **extra regressors** to the model: in our case it is likely to be climate / weather variables, and the formulation of the model makes is relatively easy to quantify the added value derived from incorporating climate / weather explanatory variables to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to know more about the **Generalized Additive Model** framework, a good start is:   \n",
    "    \n",
    "[https://multithreaded.stitchfix.com/assets/files/gam.pdf](https://multithreaded.stitchfix.com/assets/files/gam.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='https://facebook.github.io/prophet/', width=1500, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scikit learn Mean Absolute Error metrics function (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seaborn for visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read the data: cycling counts over Tamaki drive "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the data is taken from [https://awcc.mrcagney.works/](https://awcc.mrcagney.works/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IFrame(src='https://awcc.mrcagney.works/', width=1500, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for a better view of the location of the counters, we're gonna create an interactive map in the Jupyter Notebook using [https://github.com/python-visualization/folium](https://github.com/python-visualization/folium)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reads in the locations of the counters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_counters = pd.read_csv('../data/cycling_Auckland/cycling_counters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_counters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### we're only keeping the counters for cyclists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_counters = loc_counters.query(\"user_type == 'Cyclists'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we're gonna centre the map on the Tamaki Drive counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_counters.query(\"name == 'Tamaki Drive EB'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center_lat = loc_counters.query(\"name == 'Tamaki Drive EB'\").latitude.values[0]\n",
    "center_lon = loc_counters.query(\"name == 'Tamaki Drive EB'\").longitude.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### display the counters locations on an interactive map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folium.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the development version of folium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = folium.Map(\n",
    "    location=[center_lat, center_lon],\n",
    "    zoom_start=13,\n",
    "    tiles='OpenStreetMap', \n",
    "    width='80%', \n",
    ")\n",
    "\n",
    "m.add_child(folium.LatLngPopup())\n",
    "\n",
    "# marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "# loops over each row in the DataFrame holding the counters information, and \n",
    "# adds to the map \n",
    "\n",
    "for i, row in loc_counters.iterrows():\n",
    "    name = row['name']\n",
    "    lat = row.latitude\n",
    "    lon = row.longitude\n",
    "    opened = row.setup_date\n",
    "    \n",
    "    # HTML here in the pop up \n",
    "    popup = '<b>{}</b></br><i>setup date = {}</i>'.format(name, opened)\n",
    "    \n",
    "#     folium.Marker([lat, lon], popup='<i>{}</i>'.format(name), tooltip=name).add_to(marker_cluster)\n",
    "    folium.Marker([lat, lon], popup=popup, tooltip=name).add_to(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now display the map "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now reads the cycling counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the list of files (one per year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfiles = glob('../data/cycling_Auckland/cycling_counts_????.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfiles.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### loops over the list of files, reads and keep the Tamaki Drive data, and concatenate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for f in lfiles: \n",
    "    d = pd.read_csv(f, index_col=0, parse_dates=True, usecols=['datetime', 'Tamaki Drive EB','Tamaki Drive WB'])\n",
    "    l.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(l, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10, 8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seasonal cycle (30 days running window for smoothing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,6)) \n",
    "df.rolling(window=30*24, center=True).sum().groupby(df.index.dayofyear).mean().plot(ax=ax)\n",
    "ax.grid(ls=':')\n",
    "ax.set_xlabel('day of year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weekly cycle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,6)) \n",
    "df.groupby(df.index.dayofweek).mean().plot(ax=ax)\n",
    "ax.grid(ls=':')\n",
    "ax.set_xlabel('day of week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### daily cycle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hour = df.groupby(df.index.hour).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8,7))\n",
    "\n",
    "ax.plot(summary_hour.index, summary_hour.loc[:,('Tamaki Drive EB','mean')], color='b', label='Eastern Bound')\n",
    "ax.plot(summary_hour.index, summary_hour.loc[:,('Tamaki Drive WB','mean')], color='r', label='Western Bound')\n",
    "\n",
    "ax.fill_between(summary_hour.index, summary_hour.loc[:,('Tamaki Drive WB','25%')], \\\n",
    "                summary_hour.loc[:,('Tamaki Drive WB','75%')], color='coral', alpha=0.3)\n",
    "\n",
    "ax.fill_between(summary_hour.index, summary_hour.loc[:,('Tamaki Drive EB','25%')], \\\n",
    "                summary_hour.loc[:,('Tamaki Drive EB','75%')], color='steelblue', alpha=0.3)\n",
    "\n",
    "ax.legend(loc=2)\n",
    "\n",
    "ax.set_xticks(range(24));\n",
    "ax.grid(ls=':', color='0.8')\n",
    "\n",
    "ax.set_xlabel('hour of the day')\n",
    "\n",
    "ax.set_ylabel('cyclists number');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting rid of the outliers using a median filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(subplots=True, figsize=(10, 8), **{'ylim':[0, 600]});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(df, varname = 'Tamaki Drive EB', window=24, std=2): \n",
    "    \n",
    "    dfc = df.copy() \n",
    "    \n",
    "    dfc = dfc.loc[:,[varname]]\n",
    "    \n",
    "    dfc['median']= dfc[varname].rolling(window, center=True).median()\n",
    "    \n",
    "    dfc['std'] = dfc[varname].rolling(window, center=True).std()\n",
    "    \n",
    "    dfc.loc[dfc.loc[:,varname] >= dfc['median']+std*dfc['std'], varname] = np.nan\n",
    "    \n",
    "    dfc.loc[dfc.loc[:,varname] <= dfc['median']-std*dfc['std'], varname] = np.nan\n",
    "    \n",
    "    return dfc.loc[:, varname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'Tamaki Drive EB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[:,varname] = median_filter(df_filtered, varname = varname, window=window, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varname = 'Tamaki Drive WB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.loc[:,varname] = median_filter(df_filtered, varname = varname, window=window, std=std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.plot(subplots=True, figsize=(10, 8), **{'ylim':[0, 600]});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resampling at the daily time-step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.resample('1D').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we're gonna look at the cyclists count for Eastern Bound Tamaki Drive ... starting in 2011 as it is when the climate data starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_filtered.loc['2011':,['Tamaki Drive EB']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "data.plot(ax=ax)\n",
    "ax.grid(ls=':')\n",
    "f.savefig('../figures/cycling_counts_Tamaki_drive_EB.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename({'Tamaki Drive EB':'y'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining here a few utility functions to do all the 'data wrangling'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regressor(data, regressor, varname=None): \n",
    "    \n",
    "    \"\"\"\n",
    "    adds a regressor to a dataframe of targets\n",
    "    \"\"\"\n",
    "    \n",
    "    data.loc[:,varname] = regressor.loc[:,varname]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, year=2017): \n",
    "    \n",
    "    \"\"\"\n",
    "    prepare the data for ingestion by fbprophet: \n",
    "    \n",
    "    1) divide in training and test set, using the `year` parameter (int)\n",
    "    \n",
    "    2) reset the index and rename the `datetime` column to `ds`\n",
    "    \n",
    "    returns the training and test dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    data_train = data.loc[:str(year - 1),:]\n",
    "    \n",
    "    data_test = data.loc[str(year):,:]\n",
    "    \n",
    "    data_train.reset_index(inplace=True)\n",
    "    \n",
    "    data_test.reset_index(inplace=True)\n",
    "    \n",
    "    data_train = data_train.rename({'datetime':'ds'}, axis=1)\n",
    "    \n",
    "    data_test = data_test.rename({'datetime':'ds'}, axis=1)\n",
    "    \n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_verif(forecast, data_train, data_test): \n",
    "    \"\"\"\n",
    "    put together the forecast (coming from fbprophet) \n",
    "    and the overved data, and set the index to be a proper datetime index, \n",
    "    for plotting\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    forecast.index = pd.to_datetime(forecast.ds)\n",
    "    \n",
    "    data_train.index = pd.to_datetime(data_train.ds)\n",
    "    \n",
    "    data_test.index = pd.to_datetime(data_test.ds)\n",
    "    \n",
    "    data = pd.concat([data_train, data_test], axis=0)\n",
    "    \n",
    "    forecast.loc[:,'y'] = data.loc[:,'y']\n",
    "    \n",
    "    return forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_verif(verif, year=2017):\n",
    "    \"\"\"\n",
    "    plots the forecasts and observed data, the year parameters is used to highlight \n",
    "    the difference between the training and test data \n",
    "    \"\"\"\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    train = verif.loc[:str(year - 1),:]\n",
    "    \n",
    "    ax.plot(train.index, train.y, 'ko', markersize=3)\n",
    "    \n",
    "    ax.plot(train.index, train.yhat, color='steelblue', lw=0.5)\n",
    "    \n",
    "    ax.fill_between(train.index, train.yhat_lower, train.yhat_upper, color='steelblue', alpha=0.3)\n",
    "    \n",
    "    test = verif.loc[str(year):,:]\n",
    "    \n",
    "    ax.plot(test.index, test.y, 'ro', markersize=3)\n",
    "    \n",
    "    ax.plot(test.index, test.yhat, color='coral', lw=0.5)\n",
    "    \n",
    "    ax.fill_between(test.index, test.yhat_lower, test.yhat_upper, color='coral', alpha=0.3)\n",
    "    \n",
    "    ax.axvline(str(year), color='0.8', alpha=0.7)\n",
    "    \n",
    "    ax.grid(ls=':', lw=0.5)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_verif_component(verif, component='rain', year=2017): \n",
    "    \"\"\"\n",
    "    plots a specific component of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    f, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    train = verif.loc[:str(year - 1),:]\n",
    "        \n",
    "    ax.plot(train.index, train.loc[:,component], color='steelblue', lw=0.5)\n",
    "    \n",
    "    ax.fill_between(train.index, train.loc[:, component+'_lower'], train.loc[:, component+'_upper'], color='steelblue', alpha=0.3)\n",
    "    \n",
    "    test = verif.loc[str(year):,:]\n",
    "        \n",
    "    ax.plot(test.index, test.loc[:,component], color='coral', lw=0.5)\n",
    "    \n",
    "    ax.fill_between(test.index, test.loc[:, component+'_lower'], test.loc[:, component+'_upper'], color='coral', alpha=0.3)\n",
    "    \n",
    "    ax.axvline(str(year), color='0.8', alpha=0.7)\n",
    "    \n",
    "    ax.grid(ls=':', lw=0.5)\n",
    "    \n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_regressor_to_future(future, regressors_list): \n",
    "    \"\"\"\n",
    "    adds extra regressors to a `future` dataframe created by fbprophet\n",
    "    \"\"\"\n",
    "    \n",
    "    futures = future.copy() \n",
    "    \n",
    "    futures.index = pd.to_datetime(futures.ds)\n",
    "    \n",
    "    regressors = pd.concat(regressors_list, axis=1)\n",
    "    \n",
    "    futures = futures.merge(regressors, left_index=True, right_index=True)\n",
    "    \n",
    "    futures = futures.reset_index(drop = True) \n",
    "    \n",
    "    return futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splits the data into a training and test set, and returns these data frames in a format **fbprophet** can understand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = prepare_data(data, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instantiates, then fit the model to the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in **fbprophet** is to instantiate the model, it is there that you can set the `prior scales` for each component of your time-series, as well as the number of Fourier series to use to model the cyclic components.   \n",
    "\n",
    "A general rule is that larger prior scales and larger number of Fourier series will make the model more flexible, but at the potential cost of generalisation: i.e. the model might [overfit](https://en.wikipedia.org/wiki/Overfitting), learning the noise (rather than the signal) in the training data, but \n",
    "    giving poor results when applied to yet unseen data (the test data)... setting these [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) can be more an art than a science ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(mcmc_samples=300, changepoint_prior_scale=0.01, seasonality_mode='multiplicative', \\\n",
    "            yearly_seasonality=10, \\\n",
    "            weekly_seasonality=True, \\\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(data_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make the `future` dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=len(data_test), freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots the `components` of the forecast (trend + cyclic component [yearly seasonality, weekly seasonality] at this stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### put it all together with the actual observations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = make_verif(forecast, data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif(verif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scatter plot, marginal distribution and correlation between observations and modelled / predicted values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='yhat', y='y', data = verif.loc[:'2017',:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='yhat', y='y', data = verif.loc['2017':,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error (in number of cyclists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(verif.y.values, verif.yhat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## now incorporating the effects of the holidays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_calendar = pd.read_csv('../data/holidays_calendars_2011_2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_calendar.loc[:,'ISO_date'] = pd.to_datetime(holidays_calendar.loc[:,'ISO_date'], dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_calendar = holidays_calendar.dropna(axis=1, how='all').dropna(axis=0, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_calendar = holidays_calendar.loc[-holidays_calendar.notes.str.contains('Not a public'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = holidays_calendar.loc[(holidays_calendar.loc[:,'Regional'] == 0) | holidays_calendar.RGR.str.contains('Auckland'),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holtype = 'category'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if holtype == 'category': \n",
    "    holidays = holidays.loc[:,['ISO_date','holiday_category']]\n",
    "    holidays = holidays.rename({'holiday_category':'holiday'}, axis=1)\n",
    "if holtype == 'code': \n",
    "    holidays = holidays.loc[:,['ISO_date','holiday_code']]\n",
    "    holidays = holidays.rename({'holiday_code':'holiday'}, axis=1)\n",
    "else: \n",
    "    holidays = holidays.loc[:,['ISO_date','holiday']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays = holidays.rename({'ISO_date':'ds'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(mcmc_samples=300, holidays=holidays, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='multiplicative', \\\n",
    "            yearly_seasonality=10, \\\n",
    "            weekly_seasonality=True, \\\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=len(data_test), freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = make_verif(forecast, data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_verif(verif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='yhat', y='y', data = verif.loc[:'2017',:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='yhat', y='y', data = verif.loc['2017':,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(verif.y.values, verif.yhat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## incorporating the effects of weather conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv('../data/weather/Mangere_EWS_temp.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = pd.read_csv('../data/weather/Mangere_EWS_rain.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun = pd.read_csv('../data/weather/Mangere_EWS_sun.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### interpolate so that there are no missing values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = rain.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun = sun.interpolate(method='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adds the climate regressors to the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_regressors = add_regressor(data, temp, varname='temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_regressors = add_regressor(data_with_regressors, rain, varname='rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_regressors = add_regressor(data_with_regressors, sun, varname='sun')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_regressors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_regressors.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare the data and subsets (train and test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = prepare_data(data_with_regressors, 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Prophet(mcmc_samples=300, holidays=holidays, holidays_prior_scale=0.25, changepoint_prior_scale=0.01, seasonality_mode='multiplicative', \\\n",
    "            yearly_seasonality=10, \\\n",
    "            weekly_seasonality=True, \\\n",
    "            daily_seasonality=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.add_regressor('temp', prior_scale=0.5, mode='multiplicative')\n",
    "m.add_regressor('rain', prior_scale=0.5, mode='multiplicative')\n",
    "m.add_regressor('sun', prior_scale=0.5, mode='multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = m.make_future_dataframe(periods=len(data_test), freq='1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = add_regressor_to_future(future, [temp, rain, sun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = m.predict(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif = make_verif(forecast, data_train, data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif.loc[:,'yhat'] = verif.yhat.clip_lower(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif.loc[:,'yhat_lower'] = verif.yhat_lower.clip_lower(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  plot_verif(verif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(verif.y, verif.yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verif.loc[:'2017',['y','yhat']].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='yhat', y='y', data = verif.loc[:'2017',:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = sns.jointplot(x='yhat', y='y', data = verif.loc['2017':])\n",
    "plt.savefig('../figures/joint_plot_climate.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean Absolute Error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE(verif.y.values, verif.yhat.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the contribution of the different climate variables to the response variable (in percentage of the trend component, as we chose a multiplicative model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif_component(verif, component = 'rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif_component(verif, component = 'temp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f  = plot_verif_component(verif, component = 'sun')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plots the combined contribution of the climate extra-regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif_component(verif, component = 'extra_regressors_multiplicative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zoom in on the post 2016 period (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif_component(verif.loc['2016-12-31':,:], component = 'extra_regressors_multiplicative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plot_verif_component(verif.loc['2016-12-31':,:], component = 'rain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10,8))\n",
    "verif.loc['2017-01-01':'2017-04-30',['y','yhat']].plot(lw=3, ax=ax)\n",
    "ax.grid(ls=':')\n",
    "f.savefig('../figures/forecasts_obs_2017-04.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### running correlations between observed and modelled / predicted values, useful to identify when things go South "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = verif.loc[:,['y','yhat']].rolling(window=90).corr().iloc[0::2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr.index = corr.index.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(10, 8))\n",
    "corr.plot(ax=ax)\n",
    "ax.axhline(0.7, color='0.8', alpha=0.5, zorder=-1)\n",
    "ax.axhline(0.5, color='0.8', alpha=0.4, zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
